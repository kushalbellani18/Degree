{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "photographic-transition",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "static-argument",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', ...]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "brown.words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "native-republican",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'I', 'am', 'human', '.', 'I', 'lived', 'on', 'Earth', '.', 'Earth', 'is', 'located', 'in', '3rd', 'planet', 'in', 'my', 'solar', 'system', '.', 'My', 'solar', 'system', 'name', ',', 'Milky', 'way', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "text = \"Hello, I am human. I lived on Earth. Earth is located in 3rd planet in my solar system. My solar system name, Milky way.\"\n",
    "print(word_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "invalid-underground",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello, I am human.', 'I lived on Earth.', 'Earth is located in 3rd planet in my solar system.', 'My solar system name, Milky way.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "print(sent_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "empirical-laptop",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After split:  ['every', 'code', 'is', 'a', 'big', 'enigma']\n",
      "\n",
      "\n",
      "After token:  [('every', 'DT'), ('code', 'NN'), ('is', 'VBZ'), ('a', 'DT'), ('big', 'JJ'), ('enigma', 'NN')]\n",
      "\n",
      "\n",
      "After Regex:  chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "\n",
      "\n",
      "After Chunking:  (S\n",
      "  every/DT\n",
      "  (Found -> code/NN)\n",
      "  is/VBZ\n",
      "  a/DT\n",
      "  (Found -> big/JJ)\n",
      "  (Found -> enigma/NN))\n"
     ]
    }
   ],
   "source": [
    "from nltk import pos_tag\n",
    "from nltk import RegexpParser\n",
    "\n",
    "text = \"every code is a big enigma\".split()\n",
    "print(\"After split: \", text)\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "tokens_tag = pos_tag(text)\n",
    "print(\"After token: \", tokens_tag)\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "patterns = \"\"\"Found -> : {<NN.?>*<VBD.?>*<JJ.?>*<CC>?}\"\"\"\n",
    "chunker = RegexpParser(patterns)\n",
    "print(\"After Regex: \", chunker)\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "output = chunker.parse(tokens_tag)\n",
    "print(\"After Chunking: \", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "nervous-hierarchy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Learn', 'cognitive', 'computing', 'from', 'Ganpat', 'University']\n",
      "\n",
      "\n",
      "[('Learn', 'NNP'), ('cognitive', 'JJ'), ('computing', 'VBG'), ('from', 'IN'), ('Ganpat', 'NNP'), ('University', 'NNP')]\n",
      "\n",
      "\n",
      "(S\n",
      "  Learn/NNP\n",
      "  cognitive/JJ\n",
      "  computing/VBG\n",
      "  from/IN\n",
      "  Ganpat/NNP\n",
      "  University/NNP)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "text = \"Learn cognitive computing from Ganpat University\"\n",
    "tokens = nltk.word_tokenize(text)\n",
    "print(tokens)\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "tag = nltk.pos_tag(tokens)\n",
    "print(tag)\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "grammar = \"NP: {<DT>?<JJ>*<NN>}\"\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "result = cp.parse(tag)\n",
    "print(result)\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "result.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ambient-hindu",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
